{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, cast\n",
    "import os\n",
    "import random\n",
    "from utils import *\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoProcessor, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import time\n",
    "import wandb\n",
    "from scipy.special import comb, perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('%Y-%m-%d', time.localtime(time.time()))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# wandb.init(\n",
    "#     project = 'VideoReorder',\n",
    "#     name = 'shot to scene' + timestamp\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/jianghui/dataset/VideoReorder-MovieNet'\n",
    "split = 'train'\n",
    "train_data = VideoReorderMovieNetDataFolder(root=data_path, split=split, layer='clip')\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=(split == 'train'), num_workers=8, pin_memory=True, collate_fn=lambda x: x)\n",
    "split = 'val'\n",
    "val_data = VideoReorderMovieNetDataFolder(root=data_path, split=split, layer='clip')\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=32, shuffle=(split == 'train'), num_workers=8, pin_memory=True, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512,2)\n",
    ")\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/221 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m score_batch_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m clip_data \u001b[39min\u001b[39;00m batch_data: \u001b[39m#clip data\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[39m# read input data\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     features, gt \u001b[39m=\u001b[39m clip_data\n\u001b[1;32m     25\u001b[0m     \u001b[39m# get average feature\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(features)):\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "epoch = 5\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss_func.to(device)\n",
    "pred_func = ClipPairWisePred()\n",
    "pred_func.to(device)\n",
    "optim = torch.optim.AdamW(net.parameters(), lr=lr)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "for e in range(epoch):\n",
    "    print(f'epoch {e}:')\n",
    "\n",
    "    # train\n",
    "    net.train()\n",
    "    loss_epoch_list = []\n",
    "    score_epoch_list = []\n",
    "    for batch_data in tqdm(train_dataloader):\n",
    "        loss_batch_list = []\n",
    "        score_batch_list = []\n",
    "\n",
    "        for clip_data in batch_data: #clip data\n",
    "            # read input data\n",
    "            features, gt = clip_data\n",
    "            # get average feature\n",
    "            for i in range(len(features)):\n",
    "                features[i] = torch.mean(features[i], dim = 0)\n",
    "            features = torch.stack(features, dim=0)\n",
    "\n",
    "\n",
    "            # process model\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            N = len(gt)\n",
    "            loss_scene = 0\n",
    "            score_scene = 0\n",
    "            for first_frame in range(N):\n",
    "                for second_frame in range(first_frame + 1, N):\n",
    "                    output = net(features[[first_frame, second_frame],...].reshape(-1).unsqueeze(0).to(device))\n",
    "                    loss_scene += loss_func(output, torch.tensor([1]).to(device) if gt[first_frame] < gt[second_frame] else torch.tensor([0]).to(device))\n",
    "                    PRED = get_order_index(output.reshape(-1).cpu())\n",
    "                    GT = get_order_index([gt[first_frame], gt[second_frame]])\n",
    "                    # print(PRED, GT)\n",
    "                    score_scene += int(PRED == GT)\n",
    "                    # print(score_scene)\n",
    "            loss_batch_list.append(loss_scene / comb(N, 2))\n",
    "            score_batch_list.append(score_scene / comb(N, 2))\n",
    "                \n",
    "            \n",
    "        # calcuclate avearge batch\n",
    "        score_step = sum(score_batch_list) / len(score_batch_list)\n",
    "        loss_step = sum(loss_batch_list) / len(loss_batch_list)\n",
    "        loss_step.backward()\n",
    "        optim.step()\n",
    "        # caculate avearge score\n",
    "        score_epoch_list.append(score_step)\n",
    "        loss_epoch_list.append(loss_step)\n",
    "\n",
    "    score_epoch = sum(score_epoch_list) / len(score_epoch_list)\n",
    "    loss_epoch = sum(loss_epoch_list) / len(loss_epoch_list)\n",
    "    print('train loss = ', loss_epoch.item(), 'train score = ', score_epoch)  \n",
    "\n",
    "    # val\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_epoch_list = []\n",
    "        score_epoch_list = []\n",
    "        for batch_data in tqdm(val_dataloader):\n",
    "            loss_batch_list = []\n",
    "            score_batch_list = []\n",
    "\n",
    "            for shot_data in batch_data: #clip data\n",
    "                # read input data\n",
    "                features, gt = shot_data\n",
    "                \n",
    "                # get average feature\n",
    "                for i in range(len(features)):\n",
    "                    features[i] = torch.mean(features[i], dim = 0)\n",
    "                features = torch.stack(features, dim=0)\n",
    "\n",
    "                N = len(gt)\n",
    "                loss_scene = 0\n",
    "                score_scene = 0\n",
    "                for first_frame in range(N):\n",
    "                    for second_frame in range(first_frame + 1, N):\n",
    "                        output = net(features[[first_frame, second_frame],...].reshape(-1).unsqueeze(0).to(device))\n",
    "                        loss_scene += loss_func(output, torch.tensor([1]).to(device) if gt[first_frame] < gt[second_frame] else torch.tensor([0]).to(device))\n",
    "                        PRED = get_order_index(output.reshape(-1).cpu())\n",
    "                        GT = get_order_index([gt[first_frame], gt[second_frame]])\n",
    "                        # print(PRED, GT)\n",
    "                        score_scene += int(PRED == GT)\n",
    "                        # print(score_scene)\n",
    "                loss_batch_list.append(loss_scene / comb(N, 2))\n",
    "                score_batch_list.append(score_scene / comb(N, 2))\n",
    "                \n",
    "            # calcuclate avearge batch\n",
    "            score_step = sum(score_batch_list) / len(score_batch_list)\n",
    "            loss_step = sum(loss_batch_list) / len(loss_batch_list)\n",
    "\n",
    "            # caculate avearge score\n",
    "            score_epoch_list.append(score_step)\n",
    "            loss_epoch_list.append(loss_step)\n",
    "\n",
    "        score_epoch = sum(score_epoch_list) / len(score_epoch_list)\n",
    "        loss_epoch = sum(loss_epoch_list) / len(loss_epoch_list)\n",
    "        print('val loss = ', loss_epoch.item(), 'val score = ', score_epoch)\n",
    "        if score_epoch >= best_val_acc: \n",
    "            best_val_acc = score_epoch\n",
    "            torch.save(net.state_dict(), Path('./checkpoint', f'scene_to_clip_best_{timestamp}.pth'))\n",
    "            print(\"save epoch \",e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5afd70db720be3800db2de41727453e3061ee2e6ab9dd2afe0b69a2412604e4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
